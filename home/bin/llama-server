#!/bin/bash

# define models
model_0="mistral-7b-openorca.Q8_0.gguf" # template:chatml, old one used in aje analyzer
model_1="FuseO1-DeepSeekR1-QwQ-32B-Preview-Q5_K_M.gguf"
model_2="deepseek-r1-qwen-2.5-32B-ablated-Q5_K_L.gguf"
model_3="Qwen2.5-32B-DeepSeek-R1-Instruct.Q5_K_M.gguf"

# select model
model=$model_3

# select chat template
chat_template=""

# Current llama.cpp supported chat templates, it can also read it from models metadata
# (update this, as it is been developed) Last update 2025-02-03

# chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,
# exaone3, falcon3, gemma, gigachat, granite, llama2, llama2-sys,
# llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm, mistral-v1,
# mistral-v3, mistral-v3-tekken, mistral-v7, monarch, openchat, orion,
# phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr

#locations
models_dir="/home/models/"
# server path (compile and copy binaries)
llama_server="/usr/local/bin/llama/llama-server"

#hots settings
hostname=$(hostname)
port=8080

#model settings
thread_cpu=11
thread_gpu=128
context_size=10240 # too big most likely to old models?

# set extra_params
extra_params=""

# set chat_template_param
if [ -n "$chat_template" ]; then
   echo -e "\e[32m#### Using chat template: $chat_template \e[0m"
   extra_params+=" --chat-template=$chat_template"
fi

# show selected model
echo -e "\e[32m#### Selected model: $model \e[0m"

# crate full path to model
model_full_path=$models_dir
model_full_path+=$model

# startup checks and execute
if [ ! -d $models_dir ]; then
  # no models_dir
  echo -e "\e[31m#### Models dir: $models_dir, does not exist! \e[0m"
elif [ ! -f $model_full_path ]; then 
  # no model in models_dir
  echo -e "\e[31m#### Model file in: $model_full_path, does not exist! \e[0m"
else
  # start server
  echo -e "\e[34m#### Starting server http://$hostname:$port \e[0m"
  sleep 3
  $llama_server \
    --host 0.0.0.0 --port 8080 \
    -t $thread_cpu -ngl $thread_gpu \
    -c $context_size \
    -m $model_full_path \
    $extra_params
fi


