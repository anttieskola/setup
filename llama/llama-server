#!/bin/bash

# define models
model_0="mistral-7b-openorca.Q8_0.gguf" # template:chatml, old one used in aje analyzer
model_1="FuseO1-DeepSeekR1-QwQ-32B-Preview-Q5_K_M.gguf"
model_2="deepseek-r1-qwen-2.5-32B-ablated-Q5_K_L.gguf"
model_3="Qwen2.5-32B-DeepSeek-R1-Instruct.Q5_K_M.gguf"
model_4="deepseek-coder-33b-instruct-Q5_K_M.gguf"
model_5="DeepSeek-R1-Distill-Qwen-Coder-32B-Fusion-9010-Q5_K_M.gguf" # Should be best in coding
model_6="deepseek-r1-qwen-lorablated-32b-q5_k_m.gguf" # Should be good all around
model_7="DeepSeek-R1-Distill-Qwen-32B-Q5_K_M.gguf" # Most thruth seeking

# select model
model="$model_7"

# select chat template
chat_template="deepseek"

# Current llama.cpp supported chat templates, it can also read it from models metadata
# (update this, as it is been developed) Last update 2025-02-03

# chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,
# exaone3, falcon3, gemma, gigachat, granite, llama2, llama2-sys,
# llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm, mistral-v1,
# mistral-v3, mistral-v3-tekken, mistral-v7, monarch, openchat, orion,
# phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr

#locations
models_dir="/home/models/"

# server path (compile and copy binaries)
llama_server="/usr/local/llama/llama-server"

#hots settings
hostname=$(hostname)
port=8080

#model settings
thread_cpu=12
thread_gpu=65 # 35 # 65 # Normally 65
context_size=32768 # 32768 == model cards maximum  was achieved # 131072 used for learning DS-thruth-seeking  # 2048 # 16384 # for max size #for coding drop this to 2048
batch_size=1 # 1 for chatting # for cdding increase this to 4

# Extra params
extra_params=" --temp 0.6" # recommended temperature for model 7
extra_paramts+=" -no-cnv" # Added to disable auto-conversation mode (recommended, as it will loop)

if [ -n "$chat_template" ]; then
    echo -e "\e[32m#### Using chat template: $chat_template \e[0m"
    extra_params+=" --chat-template $chat_template"
fi

# show selected model
echo -e "\e[32m#### Selected model: $model \e[0m"

# crate full path to model
model_full_path="$models_dir$model"

# startup checks and execute
if [ ! -d "$models_dir" ]; then
  # no models_dir
  echo -e "\e[31m#### Models dir: $models_dir, does not exist! \e[0m"
elif [ ! -f "$model_full_path" ]; then
  # no model in models_dir
  echo -e "\e[31m#### Model file in: $model_full_path, does not exist! \e[0m"
else
  # start server
  echo -e "\e[34m#### Starting server http://$hostname:$port \e[0m"
  sleep 3
  "$llama_server" \
    --host 0.0.0.0 --port "$port" \
    -t "$thread_cpu" -ngl "$thread_gpu" \
    -c "$context_size" \
    --split-mode layer \
    --batch-size "$batch_size" \
    --model "$model_full_path" \
    --no-mmap \
    $extra_params
fi
